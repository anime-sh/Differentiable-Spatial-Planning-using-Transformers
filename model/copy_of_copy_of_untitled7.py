# -*- coding: utf-8 -*-
"""Copy of Copy of Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b1uIG8K6AUael9yAN54x2-RJJrnSFOBY
"""

!pip install pytorch_warmup

from google_drive_downloader import GoogleDriveDownloader as gdd
gdd.download_file_from_google_drive(file_id='1lORbh70-sTXvY48ARifexxLsYepDcHSx',
                                    dest_path='./navdata_input_30.npy',
                                    unzip=False)

from google_drive_downloader import GoogleDriveDownloader as gdd
gdd.download_file_from_google_drive(file_id='10WwaIDmBo2cfdHJz4O0aABN0mIbF8ByH',
                                    dest_path='./navdata_output_30.npy',
                                    unzip=False)

import numpy as np
import torch 
import torch.nn as nn 
import torch.optim as optim
import torch.nn.functional as F
from torch import autograd,Tensor
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torch.nn import TransformerEncoder, TransformerEncoderLayer
# import pytorch_warmup as warmup
import math
import random

def random_seed(seed_value, use_cuda):
    np.random.seed(seed_value)  
    torch.manual_seed(seed_value)  
    random.seed(seed_value)
    if use_cuda:
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)  
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
random_seed(42, True)

class SyntheticNavigationDataset(Dataset):

  #probably not a good idea to load 700 mb of data directly to RAM
  #current dataset has 33000 maps ,  the paper uses 100000 

  def __init__(self,x_file,y_file,n):
    self.x_list = []
    self.y_list = []
    with open(x_file,'rb') as fx, open(y_file,'rb') as fy:
      for i in range(n):
        self.x_list.append(np.load(fx))
        self.y_list.append(np.load(fy))
  
  def __len__(self):
    return len(self.x_list)

  def __getitem__(self,idx):
    x_tensor = torch.from_numpy(self.x_list[idx].astype(np.float32))
    y_np = self.y_list[idx].astype(np.float32)
    y_np = np.where(y_np<0,0,y_np)
    # y_tensor = torch.from_numpy(y_np/np.amax(y_np))
    y_tensor = torch.from_numpy(y_np)
    sample = {'x':x_tensor,'y':y_tensor}

    return sample

nav_dataset = SyntheticNavigationDataset(x_file = './navdata_input_30.npy', y_file = './navdata_output_30.npy',n = 33000)
sample = nav_dataset[10]
print(sample['y'])

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Using {device} device')

# class PositionalEncoding(nn.Module):
#     def __init__(self, d_model=900, max_len=64):
#         """
#         Args
#             d_model: Hidden dimensionality of the input.
#             max_len: Maximum length of a sequence to expect.
#         """
        # super().__init__()

        # # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs
        # pe = torch.zeros(max_len,d_model)
        # position = torch.arange(0, d_model, dtype=torch.float).unsqueeze(1)

        # div_term = torch.exp(torch.arange(
        #     0, max_len, 2).float() * (-math.log(900) / max_len))

        
        # pe[0::2,:] = torch.transpose(torch.sin(position * div_term),0,1)
        # pe[1::2,:] = torch.transpose(torch.cos(position * div_term),0,1)
        # print(pe.shape)
        
        # pe = pe.unsqueeze(0)

        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.
        # Used for tensors that need to be on the same device as the module.
        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)
        # self.register_buffer("pe", pe, persistent=False)

    # def forward(self, x):
    #     x = x + self.pe[:, : x.size(1)]
    #     return x

# pe = PositionalEncoding()
# X = torch.rand(64,900).to(device)
# print(pe(X).shape)

class PositionalEncoding(nn.Module):
  #max len is most likely C = M^2
  def __init__(self,d_model=64,max_len=900):
    super().__init__()
    pe = torch.zeros(1,d_model,max_len)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(0)
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(max_len) / d_model)).unsqueeze(1)
    pe[0,0::2,:] = torch.sin(torch.matmul(div_term,position))
    pe[0,1::2,:] = torch.cos(torch.matmul(div_term,position))

    self.register_buffer('pe',pe)
  
  def forward(self,x):
    x = x + self.pe[:x.size(0)]
    return x

# class PositionalEncodingTest(nn.Module):

#   def __init__(self,d_model=64,max_len=900):
#     super().__init__()
#     pe = torch.zeros(d_model,max_len)

#     for j in range(900):
#       for i in range(1,33):
#         pe[2*i-2,j] = math.sin(j/(900**(2*i/64)))
#       for i in range(1,33):
#         pe[2*i-1,j] = math.cos(j/(900**(2*i/64)))

#     self.pe = pe

#   def forward(self,x):
#     x = x + self.pe
#     return x

# pe = PositionalEncoding()
# X = torch.rand(64,900).to(device)
# print(pe(X))
# pet = PositionalEncoding()
# print(pet(X))

class CNNEncoding(nn.Module):

  def __init__(self):
    super().__init__()

    self.conv1 = nn.Conv2d(in_channels=2, out_channels=8, kernel_size=1)
    self.conv2 = nn.Conv2d(in_channels = 8,out_channels=64, kernel_size=1)
    self.flatten = nn.Flatten(start_dim=2)

  def init_weights(self) -> None:
    nn.init.kaiming_uniform_(self.conv1.weight.data,nonlinearity='relu')
    nn.init.kaiming_uniform_(self.conv2.weight.data,nonlinearity='relu')

  def forward(self,x) -> Tensor:
    x = F.relu(self.conv1(x))
    x = self.conv2(x)
    x = self.flatten(x)
    
    return x

class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):
    def __init__(self, optimizer, warmup, max_iters):
        self.warmup = warmup
        self.max_num_iters = max_iters
        super().__init__(optimizer)

    def get_lr(self):
        lr_factor = self.get_lr_factor(epoch=self.last_epoch)
        return [base_lr * lr_factor for base_lr in self.base_lrs]

    def get_lr_factor(self, epoch):
        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))
        if epoch <= self.warmup:
            lr_factor *= epoch * 1.0 / self.warmup
        return lr_factor

class TransformerModel(nn.Module):

  def __init__(self,d_model=64,nhead=8,d_hid=512,nlayers=5,dropout=0.1):
    super().__init__()
    self.model_type = 'Transformer'
    self.pos_encoder = PositionalEncoding()
    self.conv_encoder = CNNEncoding()
    encoder_layers = TransformerEncoderLayer(d_model,nhead,d_hid,dropout,batch_first=True)
    self.transformer_encoder = TransformerEncoder(encoder_layers,nlayers)
    self.d_model = d_model
    self.decoder = nn.Linear(d_model,1)

  def init_weights(self) -> None:
    # initrange = 0.1
    # self.encoder.weight.data.uniform_(-initrange, initrange)
    nn.init.kaiming_uniform_(self.encoder.weight.data,nonlinearity='relu')
    self.decoder.bias.data.zero_()
    # self.decoder.weight.data.uniform_(-initrange, initrange)
    nn.init.kaiming_uniform_(self.decoder.weight.data,nonlinearity='relu')

  def forward(self, src) -> Tensor:
    src = self.conv_encoder(src)
    src = self.pos_encoder(src)
    src = torch.transpose(src,1,2)
    # print(src.shape)
    output = self.transformer_encoder(src)
    # print(output.shape)
    output = F.relu(self.decoder(output))

    return output

model = TransformerModel().to(device)
criterion = nn.MSELoss()
lr = 1.0
optimizer = torch.optim.SGD(model.parameters(),lr=lr)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer,1,gamma=0.9)
# warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)
# p = nn.Parameter(torch.empty(4, 4))
# optimizer = optim.Adam([p], lr=1.0)
# lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=330, max_iters=33000)

X = torch.rand(1,2,30,30, device=device)
tes1 = model(X)
# print(tes1.shape)
# print(torch.reshape(tes1,(-1,30,30)).shape)

dataloader = DataLoader(nav_dataset,batch_size = 20, shuffle=True)

def plot_grad_flow(named_parameters):
    ave_grads = []
    layers = []
    for n, p in named_parameters:
        if(p.requires_grad) and ("bias" not in n):
            layers.append(n)
            ave_grads.append(p.grad.abs().mean())
    plt.subplot(121)
    plt.plot(ave_grads, alpha=0.3, color="b")
    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color="k" )
    plt.xticks(range(0,len(ave_grads), 1), layers, rotation="vertical")
    plt.xlim(xmin=0, xmax=len(ave_grads))
    plt.xlabel("Layers")
    plt.ylabel("average gradient")
    plt.title("Gradient flow")
    plt.grid(True)

def plot_grad_flow_v2(named_parameters):
    '''Plots the gradients flowing through different layers in the net during training.
    Can be used for checking for possible gradient vanishing / exploding problems.
    
    Usage: Plug this function in Trainer class after loss.backwards() as 
    "plot_grad_flow(self.model.named_parameters())" to visualize the gradient flow'''
    ave_grads = []
    max_grads= []
    layers = []
    for n, p in named_parameters:
        if(p.requires_grad) and ("bias" not in n):
            layers.append(n)
            ave_grads.append(p.grad.abs().mean())
            max_grads.append(p.grad.abs().max())
    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color="c")
    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color="b")
    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color="k" )
    plt.xticks(range(0,len(ave_grads), 1), layers, rotation="vertical")
    plt.xlim(left=0, right=len(ave_grads))
    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions
    plt.xlabel("Layers")
    plt.ylabel("average gradient")
    plt.title("Gradient flow")
    plt.grid(True)
    plt.legend([Line2D([0], [0], color="c", lw=4),
                Line2D([0], [0], color="b", lw=4),
                Line2D([0], [0], color="k", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])

def train(model: nn.Module) -> None:
  size = len(dataloader.dataset)
  model.train()

  for batch, sample in enumerate(dataloader):
    pred = model(sample['x'].to(device))
    sample['y'] = sample['y'].to(device)

    loss = criterion(torch.reshape(pred,(-1,30,30)),sample['y'])
    # warmup_scheduler.dampen()
    optimizer.zero_grad()
    loss.backward()
    plot_grad_flow(model.named_parameters())
    torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)
    optimizer.step()
    # lr_scheduler.step()

    if batch % 100 == 0:
      loss , current = loss.item(), batch*len(X)
      print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

def show_output():
  plt.subplot(122)
  sample = nav_dataset[10]
  sample['x'] = torch.reshape(sample['x'],(1,2,30,30))
  output = model(sample['x'].to(device))
  output = torch.reshape(output,(30,30))
  output = output.to('cpu').detach().numpy()

  plt.imshow(output, cmap='hot', interpolation='nearest')
  plt.show()

epochs = 40

for t in range(epochs):
  print(f"Epoch {t+1}\n-------------------------------")
  train(model)
  plt.show()
  show_output()
  scheduler.step()

print("Done!")

# conv_test = CNNEncoding()
# for i in conv_test.parameters():
#   print(i.size())

# def show_output(m):
#   sample = nav_dataset[m]
#   sample['x'] = torch.reshape(sample['x'],(1,2,30,30))
#   output = model(sample['x'].to(device))
#   output = torch.reshape(output,(30,30))
#   output = output.to('cpu').detach().numpy()

#   plt.imshow(output, cmap='hot', interpolation='nearest')
#   plt.show()

# show_output(12)

torch.save(model.state_dict(), 'model_weights.pth')

model.eval()